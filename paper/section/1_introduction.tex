\section{INTRODUCTION}

Reinforcement learning (RL) learns a policy that maximizes rewards through trial and error.
Although this learning paradigm appears simple and straightforward, it has proven highly effective.
This effectiveness has been clearly demonstrated in practice.
Over the past few years, RL has achieved remarkable success across diverse applications \cite{silver2017mastering}, \cite{andrychowicz2020learning}, \cite{schrittwieser2020mastering}, \cite{ouyang2022training}.
Nevertheless, deploying RL in physical real-world environments remains a major challenge.
To deploy RL-trained agents in real-world environments, two requirements must be satisfied:
first, the agent must be able to successfully accomplish the given tasks, and second, it must ensure safety and reliability.
In standard RL, such requirements are learned exclusively from reward signals, thereby necessitating careful reward engineering.
However, reward engineering is inherently difficult, and even with carefully designed rewards, the learned policy may exploit unintended loopholes (reward hacking) or remain hard to interpret, making it difficult to ensure safety and reliability \cite{amodei2016concrete}.  % loopholes: 허점
These limitations highlight the necessity of an explicit constraint function, separated from the reward, which has motivated research on constrained RL.

Constrained RL is a method that learns a policy to maximize rewards while satisfying constraints, thus enabling agents to behave safely while successfully performing tasks.
A common formulation of constrained RL specifies constraints in terms of the expected cumulative cost \cite{brunke2022safe}.
For instance, in a driving scenario, a constraint may be to minimize unintended lane intrusions (i.e., crossing into an adjacent lane), in which case the cumulative cost can be defined as the total number of lane intrusions.
In this way, the agent can satisfy the constraint in expectation and keep the total number of intrusions within an acceptable limit.
However, even a single violation at the level of an individual state---such as an unintended lane intrusion near another vehicle at a critical moment---may nevertheless lead to accidents or catastrophic failures.

This limitation highlights the need to consider state-wise constraints, which explicitly enforce constraints at the level of individual states.  % state-wise constraints(costs)를 논문에서 일관성 있게 사용
Accordingly, we introduce state-wise Lagrange multipliers to encourage the policy to satisfy safety requirements at every timestep.
Our contributions are as follows:
\begin{itemize}
    \item[$\bullet$] In contrast to most existing methods that utilize a single Lagrange multiplier for cumulative cost constraints, our approach addresses state-wise constraints, which inherently demand state-wise multipliers.
    To this end, neural networks are utilized to approximate these multipliers, enabling adaptive enforcement of safety at the level of individual states.
    \item[$\bullet$] The proposed method allows for finer specification of constraints and achieves better constraint satisfaction compared to existing constrained RL approaches under the same training budget.
    \item[$\bullet$] The learned Lagrange multiplier network can be utilized during deployment to assess state-wise safety, providing interpretability and insight into the agent’s behavior.
\end{itemize}
