\section{CONCLUSIONS}

In this work, we proposed a method that introduces state-wise Lagrange multipliers to learn policies that satisfy constraints defined \textcolor{red}{over} state-wise costs.
Unlike most existing approaches that address constraints based on cumulative costs, our method handles state-wise constraints, which inherently require state-dependent multipliers.
To this end, neural networks are employed to approximate these multipliers, enabling the policy to more effectively satisfy constraints at the level of individual states.
\textcolor{red}{The proposed approach allows for finer specification of constraints and demonstrates consistent satisfaction across experiments.}
Moreover, the learned Lagrange multiplier network can also be utilized at deployment to assess state-wise safety, thereby providing interpretability and insight into the agentâ€™s behavior.

Building on \textcolor{red}{these} findings, future research could focus on two directions.
One \textcolor{red}{direction} is to pursue deterministic safety guarantees, for instance by leveraging the learned Lagrange multiplier network together with mechanisms such as safety filters or control barrier functions.
Another important direction is to improve sample efficiency, for example through off-policy or model-based approaches.

% This work demonstrates the potential of state-wise constrained RL and indicates that our approach could provide a promising foundation for developing safer and more interpretable RL agents.
\textcolor{red}{Overall, this work demonstrates the potential of state-wise constrained RL and suggests that our approach provides a promising foundation for developing safer and more interpretable RL agents.}
These contributions are expected to inspire further research on bridging theoretical safety guarantees and practical RL applications.