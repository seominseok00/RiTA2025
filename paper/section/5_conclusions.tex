\section{CONCLUSIONS}

In this work, we proposed a method that introduces state-wise Lagrange multipliers to learn policies that satisfy constraints defined in terms of state-wise costs.
Unlike most existing approaches that address constraints based on cumulative costs, our method handles state-wise constraints, which inherently require state-dependent multipliers.
To this end, neural networks are employed to approximate these multipliers, enabling the policy to more effectively satisfy constraints at the level of individual states.
The proposed approach allows constraints to be specified more finely and demonstrates consistent satisfaction of them.
Moreover, the learned Lagrange multiplier network can also be utilized at deployment to assess state-wise safety, thereby providing interpretability and insight into the agentâ€™s behavior.

Building on the current findings, future research could focus on two directions.
One is to pursue deterministic safety guarantees, for instance by leveraging the learned Lagrange multiplier network together with mechanisms such as safety filters or control barrier functions.
Another important direction is to improve sample efficiency, for example through off-policy or model-based approaches.

This work demonstrates the potential of state-wise constrained RL and indicates that our approach could provide a promising foundation for developing safer and more interpretable RL agents.
These contributions are expected to inspire further research on bridging theoretical safety guarantees and practical RL applications.