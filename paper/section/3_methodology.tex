
% ! Comment 1: Clarify and align the SCMDP problem statement with the implemented objective and the evaluation metric.
% ! => implemented objective에서는 advantage로 정의했는데, evaluation metric에서는 return, cost return으로 정의해서 그런듯.
% ! => objective에서는 state-wise cost로 정의했는데 evaluation metric에서는 cumulative cost로 정의


% 이전 formulation에서는 reward로 정의했는데, objective에서는 advantage로 정의해서 그런듯. (게다가 advantage는 q function과 value function의 차이로 정의되는데, q function과 value function은 현재 상태로부터 앞으로 받을 return의 기댓값이므로(누적해서 발생한 값) SCMD formulation과 완전히 일치하지 않음 - SCMDP는 현재 상태에서 발생한 cost로 정의되므로)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ! Comment 2: The authors should also pay attention to the notations, to make sure they are consistent throughout the entire paper.

% ! => formulation까지는 m개의 제약 조건을 기준으로 설명하다가 PPO Lagrangian, Proposed Method에서는 단일 제약 조건으로 설명해서 그런듯.

% ! => w/w_i, lambda의 scalar/vector 표기
% ! => PPO Lagrangian과 Proposed method의 objective에서 cost advantage를 모두 A_c로 표기

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ! Comment 3: The authors may also consider provide a brief theoretical discussion on existence of saddle points with state-dependent multipliers and function approximation.



\section{Methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Preliminaries}

\subsubsection{Problem Formulations}

\paragraph{\textbf{Markov Decision Processes}}

% ! 그냥 In RL이라고 써도 되는건지?
In RL, problems are typically formulated as a Markov decision process (MDP) \cite{sutton1998reinforcement}.
An MDP is defined as a tuple $\langle S, A, P, R, \gamma \rangle$, where $S$ and $A$ denote the state and action spaces, $P$ is the transition probability, $R$ is the reward function, and $\gamma \in [0, 1)$ is the discount factor.
The objective of RL is to find an optimal policy $\pi^*$ that maximizes the cumulative reward defined as:
\begin{equation} \label{eq:mdp_optimization_problem}
    \begin{aligned}
        J_R(\pi) &= \mathbb{E}_{\tau \sim \pi}\!\left[\sum^\infty_{t = 0} \gamma^t R(s_t, a_t)\right] \\
        \pi^* &= \arg \max J_R(\pi)
    \end{aligned}
\end{equation}
Here, $\tau = (s_0, a_0, s_1, a_1, \ldots)$ denotes a trajectory \textcolor{red}{under} policy $\pi$.

\paragraph{\textbf{Constrained Markov Decision Processes}}

In contrast, constrained RL is typically formulated as a constrained Markov decision process (CMDP) \cite{altman2021constrained}.
\textcolor{red}{A CMDP extends an MDP by introducing cost functions $C_1, \ldots, C_m$ (separate from the reward function) and their corresponding limits $d_1, \ldots, d_m$.}
Formally, a CMDP is defined as a tuple $\langle S, A, P, R, C, d, \gamma \rangle$.
In a CMDP, the set of feasible policies $\Pi_C$ is defined as:
\begin{equation} \label{eq:feasible_policy_set_cmdp}
    \begin{aligned}
        \Pi_C = \{ \pi \in \Pi: \forall i \in \{1, \ldots, m\}, \; J_{C_i}(\pi) \leq d_i \}.
    \end{aligned}
\end{equation}
Specifically, the constraint function for constraint $i$ is defined as the expected cumulative discounted cost:
\begin{equation} \label{eq:cost_return}
    J_{C_i}(\pi) = \mathbb{E}_{\tau \sim \pi}\!\left[\sum^\infty_{t = 0} \gamma^t C_i(s_t, a_t)\right], 
    \quad \forall i \in \{1, \ldots, m\}.
\end{equation}
In standard RL, the objective is an unconstrained policy optimization problem that aims to find a policy maximizing the expected return, as defined in Eq.~\eqref{eq:mdp_optimization_problem}, \textcolor{red}{whereas constrained RL seeks to maximize the expected return subject to the cost constraints defined in Eq.~\eqref{eq:cost_return}.}
\begin{equation} \label{eq:cmdp_optimization_problem}
    \pi^* = \arg\max_\pi J_R(\pi) \; \text{s. t.} \; \pi \in \Pi_C.
\end{equation}


\paragraph{\textbf{State-wise Constrained Markov Decision Process}}

The CMDP framework can be extended to \textcolor{red}{incorporate cost-based constraints of different forms.}
One such extension is the state-wise constrained Markov decision process (SCMDP) \cite{zhao2023state}, which \textcolor{red}{enforces constraints that bound the expected cost at each state by a specified threshold.}
In a SCMDP, the set of feasible policies $\Pi_{SC}$ is defined as:
\begin{equation} \label{eq:feasible_policy_set_scmdp}
    \begin{aligned}    
        \Pi_{SC} = \{ \pi \in \Pi: &\;\forall i \in \{1, \ldots, m\}, \; J_{{SC}_i}(\pi) \leq w_i \}.
    \end{aligned}
\end{equation}
In this definition, the state-wise constraint represents the expected cost incurred at each state:
\begin{equation} \label{eq:statewise_cost_return}
    J_{{SC}_i}(\pi) = \mathbb{E}_{(s_t, a_t, s_{t + 1}) \sim \tau, \, \tau \sim \pi} \big[ C_i(s_t, a_t, s_{t + 1}) \big], 
    \quad \forall i \in \{1, \ldots, m\}.
\end{equation}
Consequently, similar to CMDP, the optimization problem in SCMDP can be formulated as:
\begin{equation} \label{eq:scmdp_optimization_problem}
    \pi^* = \arg\max_\pi J_R(\pi) \; \text{s.t.} \; \pi \in \Pi_{SC}.
\end{equation}  % refrence: https://arxiv.org/pdf/2306.12594
\textcolor{red}{In a CMDP, constraints are imposed on the cumulative cost, whereas an SCMDP enforces constraints on the expected cost at each state.}
\textcolor{red}{This enables safety guarantees at the state level, as illustrated in Fig.~\ref{fig:constrained_rl_vs_statewise_constrained_rl}.}
% TODO: cost 계산만 다르게 할거면 그림은 통일
% TODO: 그림 quality가 낮음
% TODO: caption의 설명을 어디서 할건지? 지금까지 설명한 내용으로 충분하지 않은가요?
\begin{figure}[H]
    \centering

    % (a) Constrained RL
    \begin{subfigure}{0.46\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/constrained-rl.pdf}
        \caption{Constrained RL}
    \end{subfigure}
    \hfill
    % (b) State-wise constrained RL
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/statewise-constrained-rl.pdf}
        \caption{State-wise constrained RL}
    \end{subfigure}
    \caption{Comparison of constrained RL and state-wise constrained RL.
            In constrained RL, a policy is feasible if the cumulative cost is below the limit.
            In state-wise constrained RL, a policy is feasible if the cost at every state is below the limit.}
    \label{fig:constrained_rl_vs_statewise_constrained_rl}
\end{figure}



\subsubsection{Lagrangian Relaxation for Constrained Policy Optimization} \label{subsubsec:lagrangian_relaxation}

A common approach to solve constrained optimization problems is \textcolor{red}{to use the Lagrangian relaxation method.}
In this approach, the constrained optimization problem \eqref{eq:cmdp_optimization_problem} is reformulated as an unconstrained optimization problem by introducing Lagrange multiplier $\lambda \geq 0$ that penalizes constraint violations.
The resulting Lagrangian can be written as:
\begin{equation}
    L(\theta, \lambda) = J_R(\pi_\theta) - \lambda^\top (J_C(\pi_\theta) - d),
\end{equation}
where $\theta$ denotes the parameter of the policy.
The objective is then to find a saddle point $(\theta^*, \lambda^*)$ that satisfies:
\begin{equation}
    L(\theta^*, \lambda) \geq L(\theta^*, \lambda^*) \geq L(\theta, \lambda^*).
\end{equation}
Since finding a global saddle point is \textcolor{red}{typically} intractable, in practice, one seeks a locally optimal solution by iteratively updating the policy parameters and the Lagrange multipliers.
A common approach is to apply gradient-based updates of the form
\begin{align}
    \theta_{n+1} &= \theta_n + \eta_\theta \nabla_\theta \Big(J_R(\pi_\theta) - \lambda_n^\top J_C(\pi_\theta)\Big), \\
    \lambda_{n+1} &= \Big[ \lambda_n + \eta_\lambda \big( J_C(\pi_\theta) - d \big) \Big]_+,
\end{align}
where $\eta_\theta, \eta_\lambda > 0$ are step sizes, and $[\cdot]_+$ denotes the projection onto the nonnegative reals to ensure $\lambda \geq 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PPO Lagrangian
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Formulations of PPO under MDP and CMDP}

\paragraph{\textbf{PPO}}

Among policy gradient methods, PPO is one of the most widely used algorithms, proposed to solve the optimization problem in Eq.~\eqref{eq:mdp_optimization_problem}.
To improve stability, PPO introduces a clipping mechanism that prevents large policy updates.
The objective function of PPO is defined as:
\begin{equation} \label{eq:ppo_objective}
    J^{\text{PPO}}(\theta) = \mathbb{E}_{\pi_{\theta_\text{old}}} \left[ \min \left( r(\theta) A^{\pi_{\theta_\text{old}}}_R (s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}_R (s, a) \right) \right]
\end{equation}
where $r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_\text{old}}(a|s)}$ denotes the probability ratio between the current and the old policies, and $A^{\pi_{\theta_\text{old}}}_R (s, a) = Q^{\pi_{\theta_\text{old}}}_R (s, a) - V^{\pi_{\theta_\text{old}}}_R (s)$ represents the \textit{reward advantage function} under the old policy.

\paragraph{\textbf{PPO Lagrangian}}

PPO Lagrangian extends PPO to the constrained RL setting, allowing the algorithm to handle explicit constraints.
The original PPO algorithm enhances stability by limiting policy updates through clipping, as defined in Eq.~\eqref{eq:ppo_objective}, but it does not \textcolor{red}{explicitly account for constraints.}
To address constraints defined in terms of the cumulative cost in Eq.~\eqref{eq:cost_return}, PPO Lagrangian applies the Lagrangian relaxation technique, as discussed in Section~\ref{subsubsec:lagrangian_relaxation}, thereby converting the constrained optimization problem in Eq.~\eqref{eq:cmdp_optimization_problem} into an unconstrained one.
\textcolor{red}{For simplicity, we consider a single-constraint case throughout this paper, although the original formulation supports multiple constraints.}
\begin{equation} \label{eq:ppo_lagrangian_objective}
    \begin{aligned} J^{\text{PPO-Lag}}(\theta)
        = \mathbb{E}_{\pi_{\theta_\text{old}}} \Big[ &\min \big( r(\theta) A^{\pi_{\theta_\text{old}}}_R (s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}_R (s, a) \big)
        \\ &- \lambda r(\theta) A^{\pi_{\theta_\text{old}}}_C (s, a) \Big]
    \end{aligned}
\end{equation}
where $\lambda \geq 0$ is the Lagrange multiplier that imposes a penalty on constraint violations, and \textcolor{red}{$A^{\pi_{\theta_\text{old}}}_C(s, a) = Q^{\pi_{\theta_\text{old}}}_C(s, a) - V^{\pi_{\theta_\text{old}}}_C(s)$ denotes the \textit{cost advantage function} under the old policy.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proposed Method}


% TODO: 이 figure를 쓸거면 본문에서 언급해야 함
% TODO: figure quality가 낮음

\begin{figure}[h]
    \centering

    \includegraphics[width=0.8\linewidth]{figure/ppo_lagnet.pdf}
    \caption{Overview of the proposed method.
            Unlike standard PPO-Lagrangian, which employs a scalar Lagrange multiplier \textcolor{red}{since} the constraint is defined on cumulative cost, our approach imposes constraints in a state-wise manner, requiring state-varying multipliers. 
            To estimate these multipliers, we introduce an additional neural network, termed the Lagrange multiplier network.}
    \label{fig:ppo_lagnet}
\end{figure}

% TODO: PPO Lagrangian에 대한 설명이 없음
% TODO: PPO Lagrangian과의 차이를 명확히 언급 (단순히 NN을 썼다고 한 것만으로는 불충분)
In this paper, we propose a method that extends PPO Lagrangian to the state-wise constrained RL setting by estimating state-wise Lagrange multipliers.
Similar to PPO Lagrangian in Eq.~\eqref{eq:ppo_lagrangian_objective}, which employs a scalar Lagrange multiplier to handle constraints on the cumulative cost, addressing the state-wise constraint defined in Eq.~\eqref{eq:statewise_cost_return} requires a state-varying multiplier.
To this end, we introduce an additional neural network, referred to as the Lagrange multiplier network, which maps \textcolor{red}{each} state to its corresponding Lagrange multiplier, as illustrated in Fig.~\ref{fig:ppo_lagnet}.
Formally, the Lagrange multiplier network is parameterized by $\xi$ and is denoted as $\lambda_\xi(s)$.
This enables the application of Lagrangian relaxation to the constrained policy optimization defined in Eq.~\eqref{eq:scmdp_optimization_problem}, analogous to the way PPO-Lagrangian handles cumulative cost constraints.
The objective function of the proposed method can thus be expressed as:
\begin{equation} 
    \begin{aligned} J^{\text{Proposed}}(\theta) 
        = \mathbb{E}_{\pi_{\theta_\text{old}}} \Big[ &\min \big( r(\theta) A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}(s, a) \big) 
        \\ &- \lambda_\xi(s) r(\theta) A^{\pi_{\theta_\text{old}}}_c \Big] 
    \end{aligned} 
\end{equation}
The parameters of the Lagrange multiplier network are updated using the following objective:
\begin{equation} \label{eq:lagrange_multiplier_update}
    J_\lambda(\xi) = \mathbb{E}_{(s_t, a_t, s_{t + 1}) \sim \tau, \tau \sim \pi_{\theta_\text{old}}} [\lambda_\xi(s_t) (C(s_t, a_t, s_{t + 1}) - w)]
\end{equation}
where $w$ denotes the state-wise cost limit.  % ! 여기서도 w로 표기했는데, 이럴 경우 설명의 편의를 위해 constraint가 하나라고 언급하는 방향으로 수정하는게 좋을듯?
In this formulation, the objective $J_\lambda(\xi)$ \textcolor{red}{encourages} the multiplier $\lambda_\xi(s)$ to increase when the observed cost $C(s_t, a_t, s_{t+1})$ exceeds the limit $w$, and to decrease when it falls below $w$.
