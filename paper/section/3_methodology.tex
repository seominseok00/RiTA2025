\section{Methodology}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preliminaries}

\subsubsection{Constrained Markov Decision Processes}

In RL, problems are typically formulated as Markov decision process (MDP) \cite{sutton1998reinforcement}, in contrast, constrained RL employs a constrained Markov decision process (CMDP) \cite{altman2021constrained}.
A CMDP extends an MDP by introducing a set of cost functions $C_1, \ldots, C_m$, which are separate from the reward function, together with corresponding thresholds $d_1, \ldots, d_m$.
Formally, a CMDP is defined as a tuple $\langle S, A, P, R, C, d, \gamma \rangle$, where $S$ and $A$ denotes the state and action spaces, $P$ is the transition probability, $R$ is the reward function, $C$ is the set of cost functions, and $d$ denotes the corresponding cost thresholds, $\gamma$ is the discount factor.
In a CMDP, the set of feasible policies $\Pi_C$ is defined as:
\begin{equation} \label{eq:feasible_policy_set_cmdp}
    \Pi_C = \{ \pi \in \Pi: \forall i, J_{C_i}(\pi) \leq d_i \},
\end{equation}
where $J_{C_i} = \mathbb{E}_{\tau \sim \pi}[\sum^\infty_{t = 0} \gamma^t C(s_t, a_t)]$ is a cost-based constraint function, commonly defined in the same way as the expected return.
Specifically, the expected return is defined as $J_R = \mathbb{E}_{\tau \sim \pi}[\sum^\infty_{t = 0} \gamma^t R(s_t, a_t)]$, and $d_i$ is a threshold chosen as a human-specified hyperparameter.
Here, $\tau = (s_0, a_0, s_1, a_1, \ldots)$ denotes a trajectory generated by following policy $\pi$.
In standard RL, the objective is to sove an optimization problem that maximizes the expected return $\pi^* = \arg \max J_R(\pi)$.
Whereas constrained RL solves the following constrained optimization problem:
\begin{equation} \label{eq:cmdp_optimization_problem}
    \pi^* = \arg\max_\pi J_R(\pi) \; \text{s. t.} \; J_C(\pi) \leq d.
\end{equation}

\subsubsection{State-wise Constrained Markov Decision Process}

The CMDP framework can be extended to incorporate various types of cost-based constraints.
One such extension is the state-wise constrained Markov decision process (SCMDP) \cite{zhao2023state}, which introduces state-wise constraints to ensure that the expected cost at each state does not exceed a specified threshold.
In a SCMDP, the set of feasible policies $\Pi_{SC}$ is defined as:
\begin{equation} \label{eq:feasible_policy_set_scmdp}
    \begin{aligned}    
        \Pi_{SC} = \{ \pi \in \Pi: &\; \forall (s_t, a_t, s_{t + 1}) \sim \tau, \\
                                &\; \forall i, C_i(s_t, a_t, s_{t + 1}) \leq d_i \}
    \end{aligned}
\end{equation}
where $C_i(s_t, a_t, s_{t + 1})$ is the cost incurred at state $s_t$ after taking action $a_t$ and transitioning to state $s_{t + 1}$, and $d_i$ is the corresponding threshold.
Similar to CMDPs, the optimization problem in SCMDPs can be formulated as
\begin{equation} \label{eq:scmdp_optimization_problem}
    \pi^* = \arg\max_\pi J_R(\pi) \; \text{s.t.} \; J_{SC}(\pi) \leq d ,
\end{equation}
where $J_{SC}(\pi)$ denotes the state-wise constraints.  % TODO: define $J_{SC}(\pi)$

\subsubsection{Lagrangian Relaxation for Constrained Policy Optimization}

A common approach to solve constrained optimization problems is to use Lagrangian relaxation.
In this approach, the constrained optimization problem \eqref{eq:cmdp_optimization_problem} is reformulated as an unconstrained optimization problem by introducing Lagrange multiplier $\lambda \geq 0$ that penalizes constraint violations.
The resulting Lagrangian can be written as:
\begin{equation}
    L(\theta, \lambda) = J_R(\pi_\theta) - \lambda (J_C(\pi_\theta) - d),
\end{equation}
where $\theta$ denotes the parameter of the policy $\pi_\theta$.
The objective is then to find a saddle point $(\theta^*, \lambda^*)$ that satisfies:
\begin{equation}
    L(\theta^*, \lambda) \geq L(\theta^*, \lambda^*) \geq L(\theta, \lambda^*).
\end{equation}
Since finding a global saddle point is often computationally intractable, in practice one aims to find a locally optimal solution using iterative updates of the policy parameters and the Lagrange multiplier. 
A common approach is to apply gradient-based updates of the form
\begin{align}
    \theta_{n+1} &= \theta_n + \eta_\theta \nabla_\theta \Big(J_R(\pi_\theta) - \lambda_n J_C(\pi_\theta)\Big), \\
    \lambda_{n+1} &= \Big[ \lambda_n + \eta_\lambda \big( J_C(\pi_\theta) - d \big) \Big]_+,
\end{align}
where $\eta_\theta, \eta_\lambda > 0$ are step sizes, and $[\cdot]_+$ denotes the projection onto the nonnegative orthant to ensure $\lambda \geq 0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proposed Method}

In this paper, we propose a method to extend constrained reinforcement learning algorithm to state-wise constrained reinforcement learning by estimating state-wise Lagrange multipliers.
We build our method on Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, a widely used policy gradient algorithm, due to its simplicity and effectiveness.
The objective function of PPO is defined as follows:
\begin{equation} 
    J^{\text{PPO}}(\theta) = \mathbb{E}_{\pi_{\theta_\text{old}}} \left[ \min \left( r(\theta) A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}(s, a) \right) \right] 
\end{equation}
To solve Eq. \eqref{eq:scmdp_optimization_problem}, we apply Lagrangian relaxation and reformulate the objective as follows:
\begin{equation} 
    \begin{aligned} J^{\text{PPO-Lagnet}}(\theta) 
        = \mathbb{E}_{\pi_{\theta_\text{old}}} \Big[ &\min \big( r(\theta) A^{\pi_{\theta_\text{old}}}(s, a), \text{clip}(r(\theta), 1 - \epsilon, 1 + \epsilon) A^{\pi_{\theta_\text{old}}}(s, a) \big) 
        \\ &- \lambda_\xi(s) r(\theta) A^{\pi_{\theta_\text{old}}}_c \Big] 
    \end{aligned} 
\end{equation}
In order to transform Eq. \eqref{eq:scmdp_optimization_problem} into a Lagrangian function, a separate Lagrange multiplier is required for each state because the constraints are defined in \eqref{eq:feasible_policy_set_scmdp}.
Therefore, we employ a neural network to \textcolor{red}{estimate/approximate} state-dependent Lagrange multipliers, enabling scalable and efficient policy updates.
The state-wise Lagrange multiplier network is parameterized by $\xi$ and takes the state $s$ as input, outputting the corresponding Lagrange multiplier $\lambda_\xi(s)$.
The parameters of the Lagrange multiplier network are updated according to
\begin{equation}
    \lambda_\xi(s) \leftarrow \lambda_\xi(s) + \beta(\hat{J}_{SC} - d)
\end{equation}
where $\hat{J}_{SC}$ is the empirical estimate of the state-wise constraint, and $d$ is the threshold.
In this update rule, the network's output increases when the constraint is violated ($\hat{J}_{SC} > d$) and decreases when the constraint is satisfied ($\hat{J}_{SC} < d$).