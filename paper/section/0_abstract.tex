\begin{abstract}
    Despite the remarkable success of deep reinforcement learning (deep RL) across various domains, its deployment in real-world remains limited due to safety concerns.
    To address this challenge, constrained reinforcement learning (constrained RL) has been proposed to learn safe policies while maintaining performance.
    However, since constrained RL enforces constraints in the form of cumulative costs, it cannot guarantee state-wise safety.
    In this paper, we extend the Lagrangian based approach, a representative method in constrained RL, by introducing state-dependent Lagrange multipliers so that the policy is trained to account for state-wise safety.
\end{abstract}